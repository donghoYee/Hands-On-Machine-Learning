{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9021bd97",
   "metadata": {},
   "source": [
    "## Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64556fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6f419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 22:19:57.954697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:57.979404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:57.979566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:57.980175: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-27 22:19:57.981104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:57.981247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:57.981363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:58.304439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:58.304610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:58.304739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 22:19:58.304851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7408 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc92a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5625a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419c285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repeat(3).batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b54a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b33c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x*2, num_parallel_calls=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b463cdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_1699964/643490874.py:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c9e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x : x<18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9787f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731c51d",
   "metadata": {},
   "source": [
    "#### Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93397fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e1246d",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861cb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = 1.0, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19154c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    X = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    \n",
    "    return (X-X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d11a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_cells=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_cells=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a915f1",
   "metadata": {},
   "source": [
    "### Using tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ecf35",
   "metadata": {},
   "source": [
    "```\n",
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdd665",
   "metadata": {},
   "source": [
    "```\n",
    "model = keras.models.Sequential([...])\n",
    "model.compile([...])\n",
    "model.fit(train_set, epochs=10, validation_data=valid_set) # can fit without X and y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7539ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, optimizer, loss_fn, n_epochs, **kwargs):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeate=n_epochs, **kwargs)\n",
    "    \n",
    "    for X_batch, y_batch in train_set: #이렇게 해서 분리, 배치처리 됨!\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_pred, y_batch)) # loss calculates losses for each data!\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "            pass\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e4708",
   "metadata": {},
   "source": [
    "## TFRecord Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf15ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a253300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths) # can input more than one filepaths!\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List, Features, Feature, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bd77086",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\" : Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\" : Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\" : Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
    "                                                           b\"c@d.com\"]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61aa3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71ac7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7efe1c3ed240>, 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"name\" : tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\" : tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\" : tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b2b7f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4e799",
   "metadata": {},
   "source": [
    "## 입력 특성 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2345859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(36).reshape(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fec3c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_train, axis=0, keepdims=True)\n",
    "stds = np.std(X_train, axis=0, keepdims=True)\n",
    "eps = keras.backend.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2589f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
    "    #... other layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afd29223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15. 16. 17. 18. 19. 20.]]\n",
      "[[10.24695077 10.24695077 10.24695077 10.24695077 10.24695077 10.24695077]]\n",
      "1e-07\n"
     ]
    }
   ],
   "source": [
    "print(means)\n",
    "print(stds)\n",
    "print(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea73bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole user-dev layer\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample): # to store preprocessing values\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3ba4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()\n",
    "std_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c956bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "# make model\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70259d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "model.add(norm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e7f39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cebe51d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.52140000e+00,  1.50000000e+01,  3.04994451e+00,\n",
       "         1.10654828e+00,  1.44700000e+03,  1.60599334e+00,\n",
       "         3.76300000e+01, -1.22430000e+02],\n",
       "       [ 5.32750000e+00,  5.00000000e+00,  6.49005964e+00,\n",
       "         9.91053678e-01,  3.46400000e+03,  3.44333996e+00,\n",
       "         3.36900000e+01, -1.17390000e+02],\n",
       "       [ 3.10000000e+00,  2.90000000e+01,  7.54237288e+00,\n",
       "         1.59152542e+00,  1.32800000e+03,  2.25084746e+00,\n",
       "         3.84400000e+01, -1.22980000e+02],\n",
       "       [ 7.17360000e+00,  1.20000000e+01,  6.28900256e+00,\n",
       "         9.97442455e-01,  1.05400000e+03,  2.69565217e+00,\n",
       "         3.35500000e+01, -1.17700000e+02],\n",
       "       [ 2.05490000e+00,  1.30000000e+01,  5.31245745e+00,\n",
       "         1.08509190e+00,  3.29700000e+03,  2.24438393e+00,\n",
       "         3.39300000e+01, -1.16930000e+02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8eddc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29d1057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d71f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df1b9b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "add0f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocab = ['<1H OCEAN', 'INLAND',  'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "print(indices)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices) # initialize table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "695db87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6df82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot-vector encoding\n",
    "categories = tf.constant(['NEAR OCEAN', 'NEAR BAY', 'ISLAND', \"DESERT\"]) # with one unknown\n",
    "cat_indices = table.lookup(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8fa5eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 3 4 5], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9b48fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]], shape=(4, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "print(cat_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffaede56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = keras.layers.TextVectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c50ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec.adapt(housing[\"ocean_proximity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37c37c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20640, 2), dtype=int64, numpy=\n",
       "array([[5, 6],\n",
       "       [5, 6],\n",
       "       [5, 6],\n",
       "       ...,\n",
       "       [4, 0],\n",
       "       [4, 0],\n",
       "       [4, 0]])>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec(housing[\"ocean_proximity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "790b5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dbe8185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.19123924, 0.07143486],\n",
       "       [0.20266116, 0.5653901 ],\n",
       "       [0.10385895, 0.5766828 ],\n",
       "       [0.7025596 , 0.9996774 ],\n",
       "       [0.7523098 , 0.0173403 ],\n",
       "       [0.75574756, 0.68333125],\n",
       "       [0.8093147 , 0.61791205]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dee13dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.10385895, 0.5766828 ],\n",
       "       [0.7025596 , 0.9996774 ],\n",
       "       [0.7523098 , 0.0173403 ],\n",
       "       [0.75574756, 0.68333125]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66b1093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04380808 -0.02991445]\n",
      " [ 0.01114583 -0.02278848]\n",
      " [ 0.00278368 -0.01677831]\n",
      " [ 0.00017037 -0.01561464]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embedding = keras.layers.Embedding(input_dim = len(vocab)+num_oov_buckets, output_dim=embedding_dim)\n",
    "print(embedding(cat_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f4595c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = keras.layers.Input(shape=[8])\n",
    "categories= keras.layers.Input(shape=[], dtype=tf.string)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories) # use functional API on Lambda function!\n",
    "cat_embed= keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "encoded_inputs= keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8cdbd8",
   "metadata": {},
   "source": [
    "## 케라스 전처리 층 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b46725",
   "metadata": {},
   "source": [
    "```\n",
    "normalization = keras.layers.Normalization()\n",
    "discritization = keras.layers.Discritization([...])\n",
    "pipeline = keras.layers.PreprocessingStage([normalization, discritization])\n",
    "pipeline.adapt(data_sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec36cb6",
   "metadata": {},
   "source": [
    "#### Tf transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5a0fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11826538",
   "metadata": {},
   "source": [
    "```\n",
    "def preprocess(inputs):\n",
    "    median_age = inputs[\"housing_median_age\"]\n",
    "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "    standard_age = tft.scale_to_z_score(median_age)\n",
    "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "    return {\n",
    "        \"standardized_median_age\" : standardized_age,\n",
    "        \"ocean_proximity_id\" : ocean_proximity_id\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc4df7",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8bf49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59bab2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a190303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1) # shuffle dataset and create batchs and load it to GPU !! CRITICAL to PERFORMANCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "726bfcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"])) # use this to map all items from dict to tuple!\n",
    "mnist_train = mnist_train.prefetch(1) # using prefetch to feed data to GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f30488",
   "metadata": {},
   "source": [
    "#### This is way more simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71f3e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
    "mnist_train = dataset[\"train\"].prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "146ebe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 9.2893 - accuracy: 0.8009\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 7.6344 - accuracy: 0.8994\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 7.0070 - accuracy: 0.9247\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 6.4630 - accuracy: 0.9400\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.9749 - accuracy: 0.9493\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.5299 - accuracy: 0.9566\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.1213 - accuracy: 0.9625\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 4.7454 - accuracy: 0.9670\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 4.3986 - accuracy: 0.9707\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 4.0781 - accuracy: 0.9737\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 3.7821 - accuracy: 0.9768\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 3.5084 - accuracy: 0.9788\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 3.2552 - accuracy: 0.9814\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 3.0207 - accuracy: 0.9833\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 2.8037 - accuracy: 0.9848\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 2.6024 - accuracy: 0.9863\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 2.4160 - accuracy: 0.9877\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 2.2436 - accuracy: 0.9890\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 2.0841 - accuracy: 0.9898\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 1.9364 - accuracy: 0.9907\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.7996 - accuracy: 0.9916\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.6729 - accuracy: 0.9923\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.5556 - accuracy: 0.9930\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.4471 - accuracy: 0.9936\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.3466 - accuracy: 0.9941\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.2533 - accuracy: 0.9947\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.1671 - accuracy: 0.9951\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.0873 - accuracy: 0.9954\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0135 - accuracy: 0.9956\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.9451 - accuracy: 0.9960\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.8818 - accuracy: 0.9962\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.8232 - accuracy: 0.9964\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.7689 - accuracy: 0.9965\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.7184 - accuracy: 0.9968\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6717 - accuracy: 0.9969\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6284 - accuracy: 0.9971\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5882 - accuracy: 0.9973\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5510 - accuracy: 0.9974\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5165 - accuracy: 0.9974\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4845 - accuracy: 0.9975\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4548 - accuracy: 0.9976\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4272 - accuracy: 0.9977\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4017 - accuracy: 0.9977\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3780 - accuracy: 0.9979\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3559 - accuracy: 0.9979\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3355 - accuracy: 0.9979\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3166 - accuracy: 0.9980\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2990 - accuracy: 0.9981\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2827 - accuracy: 0.9981\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2676 - accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efaf0284070>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.L2(0.01), kernel_constraint=keras.constraints.MaxNorm(5)),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.L2(0.01), kernel_constraint=keras.constraints.MaxNorm(5)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.L2(0.01), kernel_constraint=keras.constraints.MaxNorm(5)),\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "model.fit(mnist_train, epochs=50) # only calling this will train the dataset with prefetch included!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bb3f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnist_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4140e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HOML]",
   "language": "python",
   "name": "conda-env-HOML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
