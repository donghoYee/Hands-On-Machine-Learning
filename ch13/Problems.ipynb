{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe7105e",
   "metadata": {},
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b70bc",
   "metadata": {},
   "source": [
    "## 9.\n",
    "### a.\n",
    "_문제: (10장에서 소개한) 패션 MNIST 데이터셋을 적재하고 훈련 세트, 검증 세트, 테스트\n",
    "세트로 나눕니다. 훈련 세트를 섞은 다음 각 데이터셋을 TFRecord 파일로 저장합니\n",
    "다. 각 레코드는 두 개의 특성을 가진 `Example` 프로토콜 버퍼, 즉 직렬화된 이미지(`tf.io.serialize_tensor()`를 사용해 이미지를 직렬화하세요)와 레이블입니다. 참고: 용량이 큰 이미지일 경우 `tf.io.encode_jpeg()`를 사용할 수 있습니다. 많은 공간을 절약할 수 있지만 이미지 품질이 손해를 봅니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a005f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c88ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(X_train_full, y_train_full) , (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051049f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946faa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e656667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(48000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9364bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5662324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.io import serialize_tensor\n",
    "\n",
    "X_train_serialized = serialize_tensor(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45d4a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, Int64List, Features, Feature, Example\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = serialize_tensor(image)\n",
    "    return Example(\n",
    "        features = Features(\n",
    "            feature={\n",
    "                \"image\":Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\":Feature(int64_list=Int64List(value=[label])),\n",
    "            }))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f6955e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\002\\000\\024\\225\\335\\306\\267\\256\\251\\253\\255\\277\\306\\337\\274\\247\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\004\\000\\026\\347\\370\\343\\300\\327\\341\\341\\340\\314\\344\\331\\334\\361\\363\\3422\\000\\004\\000\\000\\000\\000\\000\\000\\000\\000\\000\\317\\363\\343\\334\\343\\333\\314\\246\\263\\300\\273\\317\\331\\343\\323\\363\\267\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000,\\376\\333\\341\\364\\302\\334\\363\\264\\231\\314\\346\\302\\304\\353\\353\\326\\276\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000S\\377\\307\\353\\364\\306\\326\\344\\315\\275\\347\\354\\352\\327\\360\\311\\314\\366\\030\\000\\000\\000\\000\\000\\000\\000\\000\\000\\222\\376\\357\\327\\332\\360\\353\\323\\330\\334\\344\\336\\353\\271\\352\\356\\310\\374<\\000\\000\\000\\000\\000\\000\\000\\000\\000\\274\\344\\362\\355\\325\\364\\353\\236\\347\\361\\351\\337\\357\\267\\343\\373\\352\\374t\\000\\000\\000\\000\\000\\000\\000\\000\\000\\344\\344\\355\\365\\352\\215\\351\\364\\343\\341\\341\\354\\352\\345\\335\\376\\353\\370\\256\\000\\000\\000\\000\\000\\000\\000\\000\\000\\375\\341\\360\\366\\377j\\263\\376\\361\\346\\327\\354\\347\\361\\256\\354\\306\\331\\333\\000\\000\\000\\000\\000\\000\\000\\000\\000\\377\\350\\363\\365\\360\\376\\366\\302\\321\\345\\365\\365\\343\\353\\345\\360\\332\\363\\374\\000\\000\\000\\000\\000\\000\\000\\000\\000\\326\\372\\364\\363\\353\\352\\324\\264\\355\\341\\321\\251\\373\\357\\350\\372\\367\\307\\367\\000\\000\\000\\000\\000\\000\\000\\000\\033\\377\\354\\363\\353\\362\\354\\355\\376\\335\\370\\346\\351\\331\\323\\356\\322\\354\\351\\377\\n\\000\\000\\000\\000\\000\\000\\000J\\375\\326\\366\\353\\322\\354\\352\\357\\263\\360\\351\\374\\356\\236\\360\\331\\354\\346\\377K\\000\\000\\000\\000\\000\\000\\000i\\375\\352\\371\\372\\361\\353\\350\\370\\325\\345\\376\\320\\271\\377\\344\\365\\375\\327\\320c\\000\\000\\000\\000\\000\\000\\000\\203\\377\\315\\300\\337\\330\\362\\345\\366\\275\\333\\362\\364\\334\\352\\354\\354\\366\\362\\377t\\000\\000\\000\\000\\000\\000\\000\\234\\377\\361\\377\\374\\344\\323\\376\\362\\377\\277F\\376\\365\\353\\335\\343\\370\\241\\377{\\000\\000\\000\\000\\000\\000\\000\\270\\377\\376\\273\\326\\366\\361\\302\\322\\367\\345\\273\\374\\334\\357\\350\\363\\376\\351\\377\\215\\000\\000\\000\\000\\000\\000\\000\\270\\352\\377\\240\\276\\371\\360\\334\\306\\364\\360\\377\\257\\272\\365\\347\\335\\273\\376\\377\\232\\000\\000\\000\\000\\000\\000\\000\\267\\334\\376\\376\\366\\344\\366\\331\\325\\366\\344\\345\\353\\376\\357\\355\\320\\337\\351\\342\\252\\000\\000\\000\\000\\000\\000\\000\\301\\377\\355\\357\\345\\351\\376\\352\\325\\357\\361\\361\\374\\226\\337\\372\\350\\376\\356\\377\\257\\000\\000\\000\\000\\000\\000\\000\\276\\375\\336\\352\\370\\323{\\377\\364\\366\\277\\264\\376\\355\\374w\\336\\375\\366\\331\\273\\000\\000\\000\\000\\000\\000\\000\\276\\367\\317\\374\\337\\353\\230\\371\\336\\361\\341\\345\\351\\342\\375a\\276\\377\\362\\360\\316\\000\\000\\000\\000\\000\\000\\000I\\324\\374\\362\\233\\320\\374\\361\\255\\352\\353\\362\\347\\211\\350\\377\\351\\366\\371\\307\\247\\000\\000\\000\\000\\000\\000\\000|\\332\\371\\360\\367\\351\\353\\360\\353\\334\\345\\347\\356\\353\\345\\347\\336\\267\\366\\351m\\000\\000\\000\\000\\000\\000\\000\\261\\353\\327ni\\377\\361\\371\\366\\343\\375\\366\\356\\376\\370\\354\\275W\\375\\363n\\000\\000\\000\\000\\000\\000\\000\\257\\377\\372Y\\000\\000\\026\\034>IBL;<\\\"\\025\\000d\\377\\377\\220\\000\\000\\000\\000\\000\\000\\000\\017\\201\\220\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\206]\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image, label in valid_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e60b5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard=index%n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "611ba254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815da7d5",
   "metadata": {},
   "source": [
    "### b.\n",
    "_문제: tf.data로 각 세트를 위한 효율적인 데이터셋을 만듭니다. 마지막으로 이 데이터셋으로\n",
    "입력 특성을 표준화하는 전처리 층을 포함한 케라스 모델을 훈련합니다. 텐서보드로 프\n",
    "로파일 데이터를 시각화하여 가능한 한 입력 파이프라인을 효율적으로 만들어보세요._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cce429e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"), #use tf.string!\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3228fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord, feature_descriptions=feature_descriptions):\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28,28])\n",
    "    return image, example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6a420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_thread=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        \n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_thread)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a48cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c4ffcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fde50f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "052267d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "class Standardization(keras.layers.Layer): # need to import layer for build\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs-self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "    \n",
    "standardization = Standardization(input_shape=[28,28]) # keras.layers.Layer has method for input_shape! -> use call to determine output shape\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image) # only output image\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()), axis=0).astype(np.float32) # concaternate 100 X 32 image batches\n",
    "print(sample_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "278fa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization.adapt(sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e872762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    standardization,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d23e8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 11:16:19.582325: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-30 11:16:19.582342: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-06-30 11:16:19.583831: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 1 GPUs\n",
      "2022-06-30 11:16:19.584028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory\n",
      "2022-06-30 11:16:19.718263: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-30 11:16:19.718454: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     49/Unknown - 1s 3ms/step - loss: 1.0091 - accuracy: 0.6518 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 11:16:20.499895: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-06-30 11:16:20.499918: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-06-30 11:16:20.570022: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-06-30 11:16:20.570203: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-06-30 11:16:20.579446: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 151 callback api events and 150 activity events. \n",
      "2022-06-30 11:16:20.581965: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-06-30 11:16:20.588985: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20\n",
      "\n",
      "2022-06-30 11:16:20.591655: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.trace.json.gz\n",
      "2022-06-30 11:16:20.600063: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20\n",
      "\n",
      "2022-06-30 11:16:20.600687: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.memory_profile.json.gz\n",
      "2022-06-30 11:16:20.600935: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20\n",
      "Dumped tool data for xplane.pb to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./my_logs/run_20220630_111619/plugins/profile/2022_06_30_11_16_20/debian.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4533 - accuracy: 0.8393 - val_loss: 0.3775 - val_accuracy: 0.8687\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3365 - accuracy: 0.8778 - val_loss: 0.3625 - val_accuracy: 0.8769\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2983 - accuracy: 0.8910 - val_loss: 0.3562 - val_accuracy: 0.8781\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2717 - accuracy: 0.8991 - val_loss: 0.3592 - val_accuracy: 0.8807\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2500 - accuracy: 0.9075 - val_loss: 0.3525 - val_accuracy: 0.8817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99844657e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "logs = os.path.join(os.curdir, \"my_logs\",\n",
    "                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set,\n",
    "          callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "190f3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "# for tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbb04e",
   "metadata": {},
   "source": [
    "## 10.\n",
    "_문제: 이 연습문제에서 데이터셋을 다운로드 및 분할하고 `tf.data.Dataset` 객체를 만들어 데이터를 적재하고 효율적으로 전처리하겠습니다. 그다음 `Embedding` 층을 포함한 이진 분류 모델을 만들고 훈련시킵니다._\n",
    "\n",
    "### a.\n",
    "_문제: [인터넷 영화 데이터베이스](https://imdb.com/)의 영화 리뷰 50,000개를 담은 [영화 리뷰\n",
    "데이터셋](https://homl.info/imdb)을 다운로드합니다. 이 데이터는\n",
    "`train`과 `test`라는 두 개의 디렉터리로 구성되어 있습니다. 각 디렉터리에는 12,500개의 긍정 리뷰를 담은 `pos` 서브디렉터리와 12,500개의 부정 리뷰를 담은 `neg` 서브디렉터리가 있습니다. 리뷰는 각각 별도의 텍스트 파일에 저장되어 있습니다. (전처리된 BOW를 포함해) 다른 파일과 디렉터리가 있지만 이 연습문제에서는 무시합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c7123b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 11s 0us/step\n",
      "84140032/84125825 [==============================] - 11s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dongho/.keras/datasets/aclImdb')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "FILENAME = \"aclImdb_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
    "path = Path(filepath).parent / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fe90925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "    train/\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "    test/\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n"
     ]
    }
   ],
   "source": [
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts) #number of directories shown\n",
    "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print(\"    \" * (indent + 1) + \"...\")\n",
    "            break\n",
    "        print(\"    \" * (indent + 1) + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e464e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")] # return postition for all text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe62cc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos = review_paths(path / \"train\" / \"pos\") # all the pathes in list!\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086587e",
   "metadata": {},
   "source": [
    "### b.\n",
    "_문제: 테스트 세트를 검증 세트(15,000개)와 테스트 세트(10,000개)로 나눕니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b85eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)\n",
    "np.random.shuffle(test_valid_neg)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad7b41",
   "metadata": {},
   "source": [
    "### c.\n",
    "_문제: tf.data를 사용해 각 세트에 대한 효율적인 데이터셋을 만듭니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92e54a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths: #filepaths is a list containing paths for texts\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices( # can input list containing string!\n",
    "        (tf.constant(reviews), tf.constant(labels))) # tf.constang transforms almost anything!\n",
    "# dataset contains tuple of reviews and labels where data is tf constant of review list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74eba36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This film has nothing whatever to do with the Sphinx, and the title is just a come-on. The story concerns an imagined true and concealed tomb in the Valley of the Kings, of King Seti I, second pharaoh of the 19th Dynasty, New Kingdom period. It is not a bad yarn, and a great deal of the film is shot on location. Even the scenes in the Winter Palace Hotel lobby in Luxor were really shot there, and not in a studio. The second unit stuff is endless, and they must have been let loose on Egypt for weeks. Frank Langella is very good indeed as a sophisticated Egyptian. He should take it up as a sideline. The film is essentially ruined by one of the world's most irritating actresses, Lesley Anne Down, who plays the lead. She spends the whole film wondering how she looks, are her blue eyes refracting light at the correct angle, do all the fellas lust after her, etc. Having started life as a model at the age of ten, what hope could there be for her? She epitomises everything that is most revolting about female vanity and dim-witted inanity. And to think that this film was directed by Franklin Shaffner, who won an Oscar for 'Patton'! He allows this terrible actress to whimper and simper through the film, hysterical one moment, flirting the next, in a kind of hurricane of idiocy as she reels from one man to another, either screaming or making bedroom eyes, it matters not. She is supposed to be a young Egyptologist. But she has never been to Egypt before! She takes a taxi to Giza and catching her first glimpse of the pyramids, gushes in ecstasy: 'But they're so BIG!!!!' Barf! OK, so that was the script, but she takes to the banality too readily, giving the impression that it is her natural element, which I don't doubt for a minute. Elements of the story are sound. There is, indeed, a serious problem about a black market in antiquities there. True! Well done! The novel by Robin Cook, which I have not seen, may be OK for all I know. It was fun to see the name of Cyril Swern as sound recordist on the film, as I knew him pretty well long ago. Stanley Kubrick's step-daughter Katharina is described as 'draughtswoman'. I wonder what that means? Maybe she did some set work. Anyway, the antiquities in the film are pretty good, actually. And we get to see lots of the Cairo Museum and numerous scenic locations. They actually go inside King Tutankhamun's Tomb! I don't imagine that would be allowed today for a movie. A lot of inappropriate scenes take place in mosques. That would not go down well today, but in 1981 such things were not on the agenda. The music for the film is absolutely appalling, worse than Lesley Anne Down in fact! But there were sound track elements which were surprisingly authentic, one being the cacophony of traffic noise of Cairo, which is accurately rendered in the background, and would make anyone who knows Cairo chuckle nervously. Also, the loudspeaker calls to prayer are there the whole time, another touch of authenticity. Why didn't they get this right? It could have been good.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"The movie had so much potential, but due to 70's technology constraints and also a weak script killed the main plot of the film. The book version of the film was much better, and well conceived. If it had been done right in the beginning with sources from the book, it could have been a very cool classic.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"I saw it, I agree with him 100%, but I didn't care for his delivery. He just came off as an asshole in a poorly edited, contrived juvenile smear campaign. Edit cuts galore, etc... The camera would be focused on him, and you'd see 2 or 3 edit cuts just over the course of a minute or two of dialog. Add in the constant boom mikes in the camera shot, which is a film no-no.<br /><br />This documentary hits a topic with so many angles, so many interesting stories, that the movie is just so easily done. Picking on religious fanatics is like picking on the retarded kid. It is so easy it is just wrong. I mean how hard is it to make these people look like nut bags? To make them contradict themselves, you just let them recite more then a verse or two. I do like when he jumped back in forth between people of the same religion and showed them completely contradicting themselves.<br /><br />I just think he could have done something a little more creative. The part with the neurologist talking about brain activity was never fleshed out. It could have been interesting to show brain scans of people during religious fits compared to drugs, or sex, or ???? He could have played more on the women all rejoicing over the Passion play that looked more like a snuff scene in a new Rob Zombie movie. More could have gone into the history of John Smith, the Mormon founder who had quite the colorful past. Delve into science v.s. religion. One is a very methodical, very strict process for increasing the confidence in theories. It builds on itself from a solid bottom up, a new layer on top of a more proved layer. An enormous burden of proof is required each step of the way. The other starts at the top and comes down with unchallengeable claims. It is so, because well\\xc2\\x85 I said so.<br /><br />Done right\\xc2\\x85 I'd say turn it into an HBO original series\\xc2\\x85 hit a different religion every week.<br /><br />It was an eye opener about one thing. I must have been blind. Good ole G.W.Bush... no wonder he got elected. He had the religious majority. And well... now that is the blind leading the blind.<br /><br />Bill Moyer.. Well.. what can I expect from a guy who hands out at Sutra in Newport beach?\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53a8516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dda53fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative, num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive, num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))  \n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1aa9e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9cff7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11360c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018632e0",
   "metadata": {},
   "source": [
    "### d.\n",
    "_문제: 리뷰를 전처리하기 위해 `TextVectorization` 층을 사용한 이진 분류 모델을 만드세요.\n",
    "`TextVectorization` 층을 아직 사용할 수 없다면 (또는 도전을 좋아한다면) 사용자 전처리 층을 만들어보세요. `tf.strings` 패키지에 있는 함수를 사용할 수 있습니다. 예를 들어 `lower()`로 소문자로 만들거나 `regex_replace()`로 구두점을 공백으로 바꾸고 `split()`로 공백을 기준으로 단어를 나눌 수 있습니다. 룩업 테이블을 사용해 단어 인덱스를 출력하세요. `adapt()` 메서드로 미리 층을 적응시켜야 합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1e4b586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
       "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
       "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
       "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>']], dtype=object)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(X_batch, n_words=50):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1,0]) + tf.constant([0, n_words])\n",
    "    Z = tf.strings.substr(X_batch, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
    "\n",
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
    "preprocess(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2aac8ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'it',\n",
       " b'great',\n",
       " b's',\n",
       " b'a',\n",
       " b'movie',\n",
       " b'i',\n",
       " b'loved',\n",
       " b'was',\n",
       " b'terrible',\n",
       " b'run',\n",
       " b'away']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b\"<pad>\":\n",
    "                counter[word] += 1\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
    "\n",
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68b7c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "        \n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57fb94e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization = TextVectorization()\n",
    "text_vectorization.adapt(X_example)\n",
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d98ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()), axis=0) # concat batches!\n",
    "\n",
    "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets, input_shape=[])\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32b6c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[  9,  14,   2,  64,  64,  12,   5, 257,   9,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  9,  13, 269, 531, 335,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8a00242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d0f8864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[2., 2., 0., 1.],\n",
       "       [2., 0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using bag-of-words to encode vectors\n",
    "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 4, 0]])\n",
    "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1) # count to word-id 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b3c10358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer): # to put in model\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:] # to get rid of <pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8ab088c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[2., 0., 1.],\n",
       "       [0., 2., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = BagOfWords(n_tokens=4)\n",
    "bag_of_words(simple_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "004d6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1\n",
    "bag_of_words = BagOfWords(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4837fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    text_vectorization, # already instantiated!\n",
    "    bag_of_words,\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8c67322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5429 - accuracy: 0.7173 - val_loss: 0.5143 - val_accuracy: 0.7376\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4715 - accuracy: 0.7680 - val_loss: 0.5033 - val_accuracy: 0.7466\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4238 - accuracy: 0.8028 - val_loss: 0.5127 - val_accuracy: 0.7452\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.3573 - accuracy: 0.8469 - val_loss: 0.5337 - val_accuracy: 0.7407\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.2772 - accuracy: 0.8942 - val_loss: 0.5662 - val_accuracy: 0.7364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99ac174220>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5c957",
   "metadata": {},
   "source": [
    "### e.\n",
    "_문제: `Embedding` 층을 추가하고 단어 개수의 제곱근을 곱하여 리뷰마다 평균 임베딩을 계산하세요(16장 참조). 이제 스케일이 조정된 이 평균 임베딩을 모델의 다음 부분으로 전달할 수 있습니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "700349bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32)) #int to float\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ac0500aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[3.535534 , 4.9497476, 2.1213205],\n",
       "       [6.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c8dbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    keras.layers.Embedding(input_dim=n_tokens,\n",
    "                           output_dim=embedding_size,\n",
    "                           mask_zero=True), # <pad> 가 영벡터가 됨\n",
    "    keras.layers.Lambda(compute_mean_embedding), # create a lambda layer like that\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafce86e",
   "metadata": {},
   "source": [
    "### f.\n",
    "_문제: 모델을 훈련하고 얼마의 정확도가 나오는지 확인해보세요. 가능한 한 훈련 속도를 빠르게 하기 위해 파이프라인을 최적화해보세요._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49f8c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5489 - accuracy: 0.7124 - val_loss: 0.5159 - val_accuracy: 0.7345\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4930 - accuracy: 0.7529 - val_loss: 0.5062 - val_accuracy: 0.7432\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4780 - accuracy: 0.7591 - val_loss: 0.5045 - val_accuracy: 0.7453\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4652 - accuracy: 0.7682 - val_loss: 0.5138 - val_accuracy: 0.7411\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4516 - accuracy: 0.7792 - val_loss: 0.5133 - val_accuracy: 0.7391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a5758d420>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952e5de",
   "metadata": {},
   "source": [
    "### g.\n",
    "_문제: `tfds.load(\"imdb_reviews\")`와 같이 TFDS를 사용해 동일한 데이터셋을 간단하게 적재해보세요._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f082a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b48bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 21:00:08.860828: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396fa60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HOML]",
   "language": "python",
   "name": "conda-env-HOML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
